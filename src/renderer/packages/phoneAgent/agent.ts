/**
 * PhoneAgent - AI-powered phone automation agent
 * TypeScript implementation of the phone agent workflow
 */

import type { BluestacksConfig } from '../bluestacksClient'
import * as bsClient from '../bluestacksClient'
import { ActionHandler, parseAction, type ActionResult } from './actionHandler'
import * as bsOps from './bluestacks'
import type { Message } from 'src/shared/types'

export interface AgentConfig {
  maxSteps?: number
  instanceId?: string
  lang?: 'cn' | 'en'
  systemPrompt?: string
  verbose?: boolean
  useChatboxModel?: boolean // If true, use Chatbox's model instead of Agent Server's LLM
  chatboxModel?: any // Chatbox model instance (ModelInterface)
  chatboxSettings?: any // Session settings for Chatbox model
  chatboxGlobalSettings?: any // Global settings for Chatbox model
}

export interface StepResult {
  success: boolean
  finished: boolean
  action?: any
  thinking?: string
  message?: string
  screenshot?: { base64Data: string; width: number; height: number }
}

export interface ModelResponse {
  thinking: string
  action: string
  rawContent: string
}

export type ConfirmationCallback = (message: string) => Promise<boolean>
export type TakeoverCallback = (message: string) => Promise<void>
export type OnStepResult = (result: StepResult) => void

/**
 * PhoneAgent - Main agent class for orchestrating phone automation
 */
export class PhoneAgent {
  private cfg: BluestacksConfig
  private agentConfig: Required<Omit<AgentConfig, 'instanceId' | 'useChatboxModel' | 'chatboxModel' | 'chatboxSettings' | 'chatboxGlobalSettings'>> & 
    { instanceId?: string; useChatboxModel?: boolean; chatboxModel?: any; chatboxSettings?: any; chatboxGlobalSettings?: any }
  private sessionId?: string
  private taskId?: string
  private actionHandler?: ActionHandler
  private stepCount = 0
  private context: Message[] = []
  private onStepResultCallback?: OnStepResult
  private confirmationCallback?: ConfirmationCallback
  private takeoverCallback?: TakeoverCallback
  private isStopped = false
  private abortController?: AbortController

  constructor(
    cfg: BluestacksConfig,
    agentConfig: AgentConfig = {},
    callbacks?: {
      onStepResult?: OnStepResult
      confirmationCallback?: ConfirmationCallback
      takeoverCallback?: TakeoverCallback
    }
  ) {
    this.cfg = cfg
    this.agentConfig = {
      maxSteps: agentConfig.maxSteps ?? 100,
      instanceId: agentConfig.instanceId ?? undefined,
      lang: agentConfig.lang ?? 'cn',
      systemPrompt: agentConfig.systemPrompt ?? this.getDefaultSystemPrompt(agentConfig.lang ?? 'cn'),
      verbose: agentConfig.verbose ?? true,
      useChatboxModel: agentConfig.useChatboxModel ?? false,
      chatboxModel: agentConfig.chatboxModel,
      chatboxSettings: agentConfig.chatboxSettings,
      chatboxGlobalSettings: agentConfig.chatboxGlobalSettings,
    }
    this.onStepResultCallback = callbacks?.onStepResult
    this.confirmationCallback = callbacks?.confirmationCallback
    this.takeoverCallback = callbacks?.takeoverCallback
  }

  /**
   * Initialize session with BlueStacks
   * 
   * When mode="agent", the Agent Server will automatically:
   * 1. Launch BlueStacks if not running
   * 2. Wait for WebSocket connection (up to 30 seconds)
   * 
   * @throws Error if session creation fails or connection timeout
   */
  async initialize(): Promise<void> {
    if (this.sessionId) {
      return // Already initialized
    }

    try {
      // First, check if server is reachable
      if (this.agentConfig.verbose) {
        console.log(`[PhoneAgent] Checking server connection: ${this.cfg.baseUrl}`)
      }
      
      let serverReachable = await bsClient.pingServer(this.cfg)
      
      // If server is not reachable, try to start it automatically
      if (!serverReachable) {
        if (this.agentConfig.verbose) {
          console.log(`[PhoneAgent] Server not reachable, attempting to start BlueStacksAI.exe...`)
        }
        
        try {
          // Try to start BlueStacksAI.exe via Electron IPC (only works in desktop app)
          const platform = await import('@/platform').then((m) => m.default)
          if (platform.type === 'desktop' && platform.startBluestacksAI) {
            const startResult = await platform.startBluestacksAI()
            if (startResult.success) {
              if (this.agentConfig.verbose) {
                console.log(`[PhoneAgent] ${startResult.message}, waiting for server to start...`)
              }
              // Wait for server to start (up to 10 seconds)
              for (let i = 0; i < 10; i++) {
                await new Promise((resolve) => setTimeout(resolve, 1000))
                serverReachable = await bsClient.pingServer(this.cfg)
                if (serverReachable) {
                  if (this.agentConfig.verbose) {
                    console.log(`[PhoneAgent] Server is now reachable after ${i + 1} seconds`)
                  }
                  break
                }
              }
            } else {
              if (this.agentConfig.verbose) {
                console.warn(`[PhoneAgent] Failed to start: ${startResult.message}`)
              }
            }
          }
        } catch (error) {
          if (this.agentConfig.verbose) {
            console.warn(`[PhoneAgent] Auto-start failed:`, error)
          }
        }
      }
      
      // Final check
      if (!serverReachable) {
        throw new Error(
          `æ— æ³•è¿æ¥åˆ° BlueStacks AI Agent Server\n\n` +
          `æœåŠ¡å™¨åœ°å€: ${this.cfg.baseUrl}\n\n` +
          `è¯·ç¡®ä¿ï¼š\n` +
          `1. BlueStacks AI Agent Server æ­£åœ¨è¿è¡Œ\n` +
          `2. æœåŠ¡å™¨åœ°å€æ­£ç¡®\n` +
          `3. é˜²ç«å¢™æœªé˜»æ­¢è¿æ¥\n\n` +
          `å¦‚ä½•å¯åŠ¨æœåŠ¡å™¨ï¼š\n` +
          `- Windows: è¿è¡Œ BlueStacksAI.exe\n` +
          `- æˆ–è®¿é—® http://localhost:8080/info æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€`
        )
      }

      if (this.agentConfig.verbose) {
        console.log(`[PhoneAgent] Server is reachable, creating session...`)
      }

      // Create session with mode="agent" to enable auto-launch
      const result = await bsClient.createSession(this.cfg, 'agent', this.agentConfig.instanceId)
      
      // Check for errors
      if (result.status === 'failure') {
        const error = result.error || 'session_creation_failed'
        const message = result.message || 'Unknown error'
        
        // Provide user-friendly error messages
        if (error === 'bluestacks_connection_failed') {
          throw new Error(
            `æ— æ³•è¿æ¥åˆ° BlueStacksã€‚è¯·ç¡®ä¿ï¼š\n` +
            `1. BlueStacks å·²å®‰è£…\n` +
            `2. BlueStacks AI Agent Server æ­£åœ¨è¿è¡Œ\n` +
            `3. ç­‰å¾… BlueStacks å®Œå…¨å¯åŠ¨åé‡è¯•\n` +
            `é”™è¯¯è¯¦æƒ…: ${message}`
          )
        }
        
        throw new Error(`Session creation failed: ${error} - ${message}`)
      }
      
      if (!result.session_id) {
        throw new Error('Failed to create session: no session_id returned')
      }
      
      this.sessionId = result.session_id
      
      if (this.agentConfig.verbose) {
        console.log(`[PhoneAgent] Session created: ${this.sessionId}`)
        console.log(`[PhoneAgent] BlueStacks should be launching automatically...`)
      }

      this.actionHandler = new ActionHandler(
        this.cfg,
        this.sessionId,
        this.confirmationCallback,
        this.takeoverCallback
      )
    } catch (error) {
      if (this.agentConfig.verbose) {
        console.error('[PhoneAgent] Session initialization failed:', error)
      }
      throw error
    }
  }

  /**
   * Run the agent to complete a task
   */
  async run(task: string, llmConfig?: any): Promise<string> {
    await this.initialize()
    this.reset()
    this.isStopped = false

    // First step with user prompt
    let result = await this.executeStep(task, true, llmConfig)

    if (this.isStopped) {
      return 'Task stopped by user'
    }

    if (result.finished) {
      return result.message || 'Task completed'
    }

    // Continue until finished or max steps reached
    while (this.stepCount < this.agentConfig.maxSteps && !this.isStopped) {
      result = await this.executeStep(undefined, false, llmConfig)

      if (this.isStopped) {
        return 'Task stopped by user'
      }

      if (result.finished) {
        return result.message || 'Task completed'
      }
    }

    if (this.isStopped) {
      return 'Task stopped by user'
    }

    return 'Max steps reached'
  }

  /**
   * Stop the agent execution
   */
  stop(): void {
    if (this.agentConfig.verbose) {
      console.log('[PhoneAgent] Stopping agent execution...')
    }
    this.isStopped = true
    
    // Abort any ongoing model calls
    if (this.abortController) {
      this.abortController.abort()
      this.abortController = undefined
    }

    // Stop task on server if taskId exists
    if (this.taskId && this.sessionId) {
      bsClient.stopTask(this.cfg, this.sessionId, this.taskId).catch((error) => {
        console.error('[PhoneAgent] Error stopping task on server:', error)
      })
    }
  }

  /**
   * Execute a single step (useful for manual control)
   */
  async step(task?: string, llmConfig?: any): Promise<StepResult> {
    await this.initialize()
    const isFirst = this.context.length === 0

    if (isFirst && !task) {
      throw new Error('Task is required for the first step')
    }

    return this.executeStep(task, isFirst, llmConfig)
  }

  /**
   * Reset agent state
   */
  reset(): void {
    this.context = []
    this.stepCount = 0
    this.taskId = undefined
  }

  /**
   * Close session and cleanup
   */
  async close(): Promise<void> {
    if (this.sessionId) {
      try {
        await bsClient.closeSession(this.cfg, this.sessionId)
      } catch (error) {
        console.error('Error closing session:', error)
      }
      this.sessionId = undefined
    }
  }

  /**
   * Get current context
   */
  getContext(): Message[] {
    return [...this.context]
  }

  /**
   * Get step count
   */
  getStepCount(): number {
    return this.stepCount
  }

  /**
   * Get session ID (for external access)
   */
  getSessionId(): string | undefined {
    return this.sessionId
  }

  /**
   * Execute a single step of the agent loop
   */
  private async executeStep(userPrompt?: string, isFirst: boolean = false, llmConfig?: any): Promise<StepResult> {
    if (!this.sessionId || !this.actionHandler) {
      throw new Error('Agent not initialized. Call initialize() first.')
    }

    // Check if stopped before executing step
    if (this.isStopped) {
      return {
        success: false,
        finished: true,
        message: 'Task stopped by user',
      }
    }

    this.stepCount++

    // Capture current screen state
    const screenshot = await bsOps.getScreenshot(this.cfg, this.sessionId)
    const currentApp = await bsOps.getCurrentApp(this.cfg, this.sessionId)

    // Build messages for model
    if (isFirst) {
      // Add system message
      this.context.push({
        id: `system-${Date.now()}`,
        role: 'system',
        contentParts: [
          {
            type: 'text',
            text: this.agentConfig.systemPrompt,
          },
        ],
      })

      // Add user message with task and screen info
      const screenInfo = this.buildScreenInfo(currentApp)
      const textContent = `${userPrompt}\n\n${screenInfo}`

      this.context.push({
        id: `user-${Date.now()}`,
        role: 'user',
        contentParts: [
          {
            type: 'text',
            text: textContent,
          },
          {
            type: 'image',
            storageKey: `screenshot-${Date.now()}`,
            // Store base64 in a way that can be used by model
            // Note: You might need to adjust this based on how Chatbox handles images
          },
        ],
      })

      // Create task via BlueStacks agent server
      if (llmConfig) {
        const taskResult = await bsClient.createTask(this.cfg, this.sessionId, userPrompt || '', llmConfig)
        this.taskId = taskResult.task_id
      }
    } else {
      // Add screen update message
      const screenInfo = this.buildScreenInfo(currentApp)
      const textContent = `** Screen Info **\n\n${screenInfo}`

      // Save screenshot to storage if using Chatbox model
      let screenshotStorageKey: string | undefined
      if (this.agentConfig.useChatboxModel) {
        const { saveScreenshotToStorage } = await import('./bluestacksTaskHandler')
        screenshotStorageKey = await saveScreenshotToStorage(screenshot.base64Data)
      }

      this.context.push({
        id: `user-${Date.now()}`,
        role: 'user',
        contentParts: [
          {
            type: 'text',
            text: textContent,
          },
          ...(screenshotStorageKey
            ? [
                {
                  type: 'image' as const,
                  storageKey: screenshotStorageKey,
                },
              ]
            : [
                {
                  type: 'image' as const,
                  storageKey: `screenshot-${Date.now()}`,
                },
              ]),
        ],
      })
    }

    // Check if stopped before getting model response
    if (this.isStopped) {
      return {
        success: false,
        finished: true,
        message: 'Task stopped by user',
      }
    }

    // Get model response
    let response: ModelResponse
    try {
      if (this.agentConfig.useChatboxModel && this.agentConfig.chatboxModel) {
        // Use Chatbox's model directly
        response = await this.getModelResponseFromChatbox(screenshot)
      } else if (this.taskId && llmConfig) {
        // Use BlueStacks agent server task streaming
        response = await this.getModelResponseFromTask()
      } else {
        // Fallback: would need to call Chatbox's model directly
        // For now, throw error to indicate this needs implementation
        throw new Error('Model response not implemented. Use Chatbox model or BlueStacks agent server with llmConfig.')
      }
    } catch (error) {
      // Check if error is due to abort
      if (error instanceof Error && error.name === 'AbortError') {
        return {
          success: false,
          finished: true,
          message: 'Task stopped by user',
        }
      }
      if (this.agentConfig.verbose) {
        console.error('Model error:', error)
      }
      return {
        success: false,
        finished: true,
        thinking: '',
        message: `Model error: ${error instanceof Error ? error.message : String(error)}`,
      }
    }

    // Check if stopped after getting model response
    if (this.isStopped) {
      return {
        success: false,
        finished: true,
        message: 'Task stopped by user',
      }
    }

    // Parse action from response
    let action: any
    try {
      action = parseAction(response.action)
    } catch (error) {
      if (this.agentConfig.verbose) {
        console.error('Parse action error:', error)
      }
      // Treat as finish action
      action = {
        _metadata: 'finish',
        message: response.action,
      }
    }

    if (this.agentConfig.verbose) {
      console.log('\n' + '='.repeat(50))
      console.log('ğŸ’­ Thinking:')
      console.log('-'.repeat(50))
      console.log(response.thinking)
      console.log('-'.repeat(50))
      console.log('ğŸ¯ Action:')
      console.log(JSON.stringify(action, null, 2))
      console.log('='.repeat(50) + '\n')
    }

    // Remove image from context to save space (keep only text)
    if (this.context.length > 0) {
      const lastMsg = this.context[this.context.length - 1]
      if (lastMsg.contentParts) {
        lastMsg.contentParts = lastMsg.contentParts.filter((p) => p.type !== 'image')
      }
    }

    // Check if stopped before executing action
    if (this.isStopped) {
      return {
        success: false,
        finished: true,
        message: 'Task stopped by user',
      }
    }

    // Execute action
    let result: ActionResult
    try {
      result = await this.actionHandler.execute(action, screenshot.width, screenshot.height)
    } catch (error) {
      if (this.agentConfig.verbose) {
        console.error('Action execution error:', error)
      }
      result = await this.actionHandler.execute(
        {
          _metadata: 'finish',
          message: error instanceof Error ? error.message : String(error),
        },
        screenshot.width,
        screenshot.height
      )
    }

    // Check if stopped after executing action
    if (this.isStopped) {
      return {
        success: false,
        finished: true,
        message: 'Task stopped by user',
      }
    }

    // Add assistant response to context
    this.context.push({
      id: `assistant-${Date.now()}`,
      role: 'assistant',
      contentParts: [
        {
          type: 'text',
          text: `<think>${response.thinking}</think><answer>${response.action}</answer>`,
        },
      ],
    })

    // Check if finished
    const finished = action._metadata === 'finish' || result.shouldFinish

    if (finished && this.agentConfig.verbose) {
      console.log('\n' + 'ğŸ‰ ' + '='.repeat(48))
      console.log(`âœ… Task completed: ${result.message || action.message || 'Done'}`)
      console.log('='.repeat(50) + '\n')
    }

    const stepResult: StepResult = {
      success: result.success,
      finished,
      action,
      thinking: response.thinking,
      message: result.message || action.message,
      screenshot: screenshot, // Include screenshot in step result to avoid duplicate calls
    }

    // Debug logging for step result
    if (this.agentConfig.verbose) {
      console.log(`[PhoneAgent] Step ${this.stepCount} result:`, {
        thinking: stepResult.thinking ? `${stepResult.thinking.substring(0, 100)}...` : '(empty)',
        thinkingLength: stepResult.thinking?.length || 0,
        action: stepResult.action,
        finished: stepResult.finished,
        hasCallback: !!this.onStepResultCallback,
      })
    }

    // Call callback if provided
    this.onStepResultCallback?.(stepResult)

    return stepResult
  }

  /**
   * Get model response from Chatbox's model
   */
  private async getModelResponseFromChatbox(screenshot: { base64Data: string; width: number; height: number }): Promise<ModelResponse> {
    if (!this.agentConfig.chatboxModel) {
      throw new Error('Chatbox model not provided')
    }

    // Import required modules
    const { streamText } = await import('@/packages/model-calls')
    const { saveScreenshotToStorage } = await import('./bluestacksTaskHandler')

    // Save screenshot to storage
    const screenshotStorageKey = await saveScreenshotToStorage(screenshot.base64Data)

    // Build messages for Chatbox model
    const messages: Message[] = [...this.context]
    
    // Update the last user message to include screenshot
    if (messages.length > 0) {
      const lastMsg = messages[messages.length - 1]
      if (lastMsg.role === 'user' && lastMsg.contentParts) {
        // Add screenshot if not already present
        const hasImage = lastMsg.contentParts.some(p => p.type === 'image')
        if (!hasImage) {
          lastMsg.contentParts.push({
            type: 'image',
            storageKey: screenshotStorageKey,
          })
        }
      }
    }

    // Call Chatbox's streamText
    let thinking = ''
    let action = ''
    let completed = false
    let fullResponseText = ''
    let lastThinking = '' // Keep track of the last extracted thinking to avoid losing it

    // Store abort controller for stop functionality
    this.abortController = new AbortController()
    const controller = this.abortController

    let streamTextResult: any = null
    
    await new Promise<void>((resolve, reject) => {
      
      streamText(
        this.agentConfig.chatboxModel,
        {
          messages,
          providerOptions: {
            ...this.agentConfig.chatboxSettings?.providerOptions,
            stream: false, // Disable streaming for easier debugging
          },
          onResultChangeWithCancel: async (result) => {
            // Check if stopped during streaming
            if (this.isStopped) {
              controller.abort()
              return
            }
            
            // IMPORTANT: result.contentParts may have been processed by extractReasoningMiddleware
            // - type: "reasoning" contains the thinking content (from <think> tags)
            // - type: "text" contains the action (from <answer> tags)
            
            // Extract thinking from reasoning parts first (processed by extractReasoningMiddleware)
            const reasoningParts = result.contentParts?.filter(p => p.type === 'reasoning') || []
            if (reasoningParts.length > 0) {
              const reasoningText = reasoningParts.map(p => (p as any).text || '').join('').trim()
              if (reasoningText && reasoningText.length > 0) {
                thinking = reasoningText
                lastThinking = reasoningText
                if (this.agentConfig.verbose) {
                  console.log('[getModelResponseFromChatbox] Extracted thinking from reasoning part:', {
                    length: thinking.length,
                    preview: thinking.substring(0, 100),
                  })
                }
              }
            }
            
            // Extract text from the response
            const textParts = result.contentParts?.filter(p => p.type === 'text') || []
            const currentText = textParts.map(p => (p as any).text || '').join('')
            fullResponseText = currentText
            
            // Debug: Log the full response text for each update (only first few characters to avoid spam)
            if (this.agentConfig.verbose && fullResponseText.length > 0) {
              console.log(`[getModelResponseFromChatbox] Response text update (length: ${fullResponseText.length}):`, {
                preview: fullResponseText.substring(0, 200),
                hasRedactedReasoning: fullResponseText.includes('<think>'),
                hasAnswer: fullResponseText.includes('<answer>'),
                hasNestedThink: fullResponseText.includes('{think}'),
                hasReasoningPart: reasoningParts.length > 0,
              })
            }
            
            // Extract action from text content (remove <answer> tags)
            if (fullResponseText.includes('<answer>')) {
              const parts = fullResponseText.split('<answer>', 2)
              if (parts[1]) {
                action = parts[1].replace(/<\/answer>/g, '').trim()
              }
            } else if (fullResponseText.trim().length > 0) {
              // If no <answer> tag, treat entire response as action (like Python version)
              action = fullResponseText.trim()
            }
            
            // Fallback: If we still don't have thinking, try to extract from fullResponseText
            // This handles cases where the middleware hasn't processed it yet
            if (!thinking && fullResponseText.includes('<think>')) {
              const parts = fullResponseText.split('<answer>', 2)
              if (parts[0]) {
                let thinkingPart = parts[0].trim()
                // Remove <think> tags
                thinkingPart = thinkingPart.replace(/<think>/g, '').replace(/<\/redacted_reasoning>/g, '').trim()
                // Remove {think} prefix if present
                if (thinkingPart.startsWith('{think}')) {
                  thinkingPart = thinkingPart.substring(7).trim()
                }
                if (thinkingPart && thinkingPart.length > 0) {
                  thinking = thinkingPart
                  lastThinking = thinkingPart
                  if (this.agentConfig.verbose) {
                    console.log('[getModelResponseFromChatbox] Extracted thinking from text (fallback):', {
                      length: thinking.length,
                      preview: thinking.substring(0, 100),
                    })
                  }
                }
              }
            }
            
            // If we haven't extracted thinking yet but have lastThinking, use it
            if (!thinking && lastThinking) {
              thinking = lastThinking
            }
            
            // Debug logging - log every time we update during streaming
            if (this.agentConfig.verbose && fullResponseText.length > 0) {
              const hasRedactedReasoning = fullResponseText.includes('<think>')
              const hasThink = fullResponseText.includes('<think>')
              const hasReasoning = fullResponseText.includes('<reasoning>')
              const hasAnswer = fullResponseText.includes('<answer>')
              const hasNestedThink = fullResponseText.includes('{think}')
              
              console.log(`[getModelResponseFromChatbox] Streaming update (length: ${fullResponseText.length}):`, {
                thinkingExtracted: thinking ? `${thinking.substring(0, 50)}...` : '(empty)',
                lastThinking: lastThinking ? `${lastThinking.substring(0, 50)}...` : '(empty)',
                actionExtracted: action ? `${action.substring(0, 50)}...` : '(empty)',
                hasRedactedReasoning,
                hasThink,
                hasReasoning,
                hasAnswer,
                hasNestedThink,
                preview: fullResponseText.substring(0, 500),
              })
            }
          },
        },
        controller.signal
      )
        .then((streamResult) => {
          completed = true
          
          // Print full API response for debugging
          console.log('==================================================')
          console.log('[Qwen API Response] http://172.16.0.61:8234/v1/chat/completions')
          console.log('==================================================')
          console.log('Full streamResult:', JSON.stringify(streamResult, null, 2))
          console.log('==================================================')
          
          // In non-streaming mode, the final result might not be in onResultChangeWithCancel
          // So we need to extract from streamResult if available
          // IMPORTANT: streamResult.contentParts may have been processed by extractReasoningMiddleware
          // - type: "reasoning" contains the thinking content (from <think> tags)
          // - type: "text" contains the action (from <answer> tags)
          if (streamResult && streamResult.contentParts) {
            if (this.agentConfig.verbose) {
              console.log('[getModelResponseFromChatbox] streamResult received:', {
                contentPartsCount: streamResult.contentParts.length,
                contentPartsTypes: streamResult.contentParts.map((p: any) => p.type),
                currentFullResponseTextLength: fullResponseText.length,
              })
            }
            
            // Print all content parts
            console.log('[Qwen API Response] Content Parts:', streamResult.contentParts.map((p: any, idx: number) => ({
              index: idx,
              type: p.type,
              text: p.text ? p.text.substring(0, 500) : '(no text)',
              fullText: p.text,
            })))
            
            // Extract thinking from reasoning parts (processed by extractReasoningMiddleware)
            const reasoningParts = streamResult.contentParts.filter((p: any) => p.type === 'reasoning') || []
            if (reasoningParts.length > 0) {
              const reasoningText = reasoningParts.map((p: any) => p.text || '').join('').trim()
              if (reasoningText && reasoningText.length > 0) {
                thinking = reasoningText
                lastThinking = reasoningText
                console.log('[Qwen API Response] Extracted thinking from reasoning part:', {
                  length: thinking.length,
                  content: thinking,
                })
              }
            }
            
            // Extract action from text parts (which contain <answer>...</answer>)
            const textParts = streamResult.contentParts.filter((p: any) => p.type === 'text') || []
            const textContent = textParts.map((p: any) => p.text || '').join('')
            
            console.log('[Qwen API Response] Extracted text content:', {
              length: textContent.length,
              content: textContent,
              hasAnswer: textContent.includes('<answer>'),
            })
            
            // Extract action from text content (remove <answer> tags)
            if (textContent.includes('<answer>')) {
              const parts = textContent.split('<answer>', 2)
              if (parts[1]) {
                action = parts[1].replace(/<\/answer>/g, '').trim()
                console.log('[Qwen API Response] Extracted action from text part:', {
                  length: action.length,
                  content: action,
                })
              }
            } else if (textContent.trim().length > 0) {
              // If no <answer> tag, use the entire text content as action
              action = textContent.trim()
            }
            
            // Also build fullResponseText for backward compatibility
            const finalText = textContent
            if (finalText && finalText.length > 0) {
              fullResponseText = finalText
            }
            
            // If we still don't have thinking but have reasoning parts, try to extract from fullResponseText
            if (!thinking && fullResponseText.includes('<think>')) {
              const parts = fullResponseText.split('<answer>', 2)
              if (parts[0]) {
                let thinkingPart = parts[0].trim()
                thinkingPart = thinkingPart.replace(/<think>/g, '').replace(/<\/redacted_reasoning>/g, '').trim()
                if (thinkingPart && thinkingPart.length > 0) {
                  thinking = thinkingPart
                  lastThinking = thinkingPart
                }
              }
            }
          }
          
          resolve()
        })
        .catch((error) => {
          if (!completed) {
            reject(error)
          } else {
            resolve()
          }
        })
    })

    // If no action was extracted, use the full response
    if (!action && fullResponseText) {
      action = fullResponseText
    }

    // Final debug logging before returning
    if (this.agentConfig.verbose) {
      console.log('[getModelResponseFromChatbox] Final result:', {
        thinkingLength: thinking?.length || 0,
        thinkingPreview: thinking?.substring(0, 100) || '(empty)',
        actionLength: action?.length || 0,
        actionPreview: action?.substring(0, 100) || '(empty)',
        fullResponseTextLength: fullResponseText.length,
      })
    }

    // Final extraction attempt after streaming is complete (using Python-style simple parsing)
    // Re-extract from fullResponseText to ensure we get the complete thinking content
    // Support multiple formats:
    // 1. <think>{think}...content...`</think>`<answer>action</answer>
    // 2. {think}...content...\n\n<answer>action</answer>
    // 3. <think>...content...`</think>`<answer>action</answer>
    if (fullResponseText && fullResponseText.includes('<answer>')) {
      const parts = fullResponseText.split('<answer>', 2)
      
      // Extract thinking: remove tags and {think} prefix if present
      let thinkingPart = parts[0].trim()
      
      // Remove <think> tags (format 1)
      thinkingPart = thinkingPart.replace(/<think>/g, '').replace(/<\/think>/g, '').trim()
      
      // Remove <think> tags (format 3) - like Python version
      thinkingPart = thinkingPart.replace(/<think>/g, '').replace(/<\/redacted_reasoning>/g, '').trim()
      
      // Remove {think} prefix if present (both formats)
      if (thinkingPart.startsWith('{think}')) {
        thinkingPart = thinkingPart.substring(7).trim()
      }
      
      if (thinkingPart && thinkingPart.length > 0) {
        thinking = thinkingPart
        lastThinking = thinkingPart
        if (this.agentConfig.verbose) {
          console.log('[getModelResponseFromChatbox] Final extraction successful:', {
            thinkingLength: thinking.length,
            thinkingPreview: thinking.substring(0, 100),
            format: fullResponseText.includes('<think>') ? 'format1 (with tags)' : 'format2 (no tags)',
          })
        }
      }
      
      // Re-extract action if not already extracted
      if (parts[1] && !action) {
        action = parts[1].replace(/<\/answer>/g, '').trim()
      }
    } else if (this.agentConfig.verbose && !thinking) {
      console.log('[getModelResponseFromChatbox] Final extraction - no <answer> tag found', {
        fullResponseTextPreview: fullResponseText.substring(0, 500),
        hasRedactedReasoning: fullResponseText.includes('<think>'),
        hasAnswer: fullResponseText.includes('<answer>'),
        hasThinkPrefix: fullResponseText.includes('{think}'),
      })
    }
    
    // Only use default thinking if we truly have no thinking content
    // If thinking is empty but we have fullResponseText, try to extract it one more time
    if (!thinking && fullResponseText) {
      // First, try using lastThinking if we have it
      if (lastThinking && lastThinking.length > 0) {
        thinking = lastThinking
      } else {
        // Last attempt: check if there's any text before the first action-like pattern
        const actionPatterns = [
          /<answer>/,
          /do\s*\(/,
          /\{[\s\S]*"action"/,
        ]
        
        let earliestActionIndex = fullResponseText.length
        for (const pattern of actionPatterns) {
          const match = fullResponseText.match(pattern)
          if (match && match.index !== undefined && match.index < earliestActionIndex) {
            earliestActionIndex = match.index
          }
        }
        
        if (earliestActionIndex > 0 && earliestActionIndex < fullResponseText.length) {
          const potentialThinking = fullResponseText.substring(0, earliestActionIndex).trim()
          // Remove any XML-like tags that might be incomplete
          const cleaned = potentialThinking
            .replace(/^<[^>]*>/, '') // Remove opening tag at start
            .replace(/<\/[^>]*>$/, '') // Remove closing tag at end
            .trim()
          if (cleaned && cleaned.length > 10) {
            thinking = cleaned
            lastThinking = cleaned
          }
        }
      }
    }
    
    // Final check: if we still don't have thinking but have lastThinking, use it
    if (!thinking && lastThinking && lastThinking.length > 0) {
      thinking = lastThinking
    }
    
    // If still no thinking, provide a default thinking message
    // This ensures every step has some thinking content displayed
    if (!thinking || thinking.length === 0) {
      // Extract action type to provide context-aware default thinking
      let actionType = 'æ‰§è¡Œæ“ä½œ'
      if (action) {
        if (action.includes('finish')) {
          actionType = 'ä»»åŠ¡å·²å®Œæˆ'
        } else if (action.includes('Launch')) {
          actionType = 'å¯åŠ¨åº”ç”¨'
        } else if (action.includes('Tap')) {
          actionType = 'ç‚¹å‡»å±å¹•å…ƒç´ '
        } else if (action.includes('Swipe')) {
          actionType = 'æ»‘åŠ¨å±å¹•'
        } else if (action.includes('Type')) {
          actionType = 'è¾“å…¥æ–‡æœ¬'
        } else if (action.includes('Back')) {
          actionType = 'è¿”å›ä¸Šä¸€é¡µ'
        }
      }
      thinking = `æ­£åœ¨åˆ†æå±å¹•çŠ¶æ€å¹¶${actionType}...`
      
      if (this.agentConfig.verbose) {
        console.log('[getModelResponseFromChatbox] Using default thinking:', thinking)
      }
    }
    
    // Log final extraction result
    if (this.agentConfig.verbose) {
      console.log('[getModelResponseFromChatbox] Final extraction:', {
        thinkingFinal: thinking ? `${thinking.substring(0, 100)}...` : '(empty)',
        lastThinkingFinal: lastThinking ? `${lastThinking.substring(0, 100)}...` : '(empty)',
        fullResponseTextPreview: fullResponseText.substring(0, 500),
        isDefaultThinking: !lastThinking && thinking.includes('æ­£åœ¨åˆ†æ'),
      })
    }

    return {
      thinking: thinking, // Always return thinking, even if it's a default message
      action: action || 'No action specified',
      rawContent: JSON.stringify({ thinking, action, fullResponseText }, null, 2),
    }
  }

  /**
   * Get model response from BlueStacks task stream
   */
  private async getModelResponseFromTask(): Promise<ModelResponse> {
    if (!this.taskId) {
      throw new Error('No task ID available')
    }

    return new Promise((resolve, reject) => {
      let thinking = ''
      let action = ''
      let completed = false

      const closeStream = bsClient.streamTask(
        this.cfg,
        this.sessionId!,
        this.taskId!,
        {
          onProgress: (data: any) => {
            // Accumulate thinking/action from progress updates
            if (data.thinking) {
              thinking += data.thinking + '\n'
            }
            if (data.action) {
              action = data.action
            }
          },
          onAwaitInput: (data: any) => {
            // Handle input requests
            console.log('Awaiting input:', data)
          },
          onCompleted: (data: any) => {
            completed = true
            if (data.thinking) {
              thinking = data.thinking
            }
            if (data.action) {
              action = data.action
            }
            resolve({
              thinking: thinking.trim(),
              action: action.trim(),
              rawContent: JSON.stringify(data),
            })
          },
          onError: (err: any) => {
            if (!completed) {
              reject(new Error(`Stream error: ${JSON.stringify(err)}`))
            }
          },
          onClose: () => {
            if (!completed) {
              reject(new Error('Stream closed before completion'))
            }
          },
        }
      )

      // Timeout after 60 seconds
      setTimeout(() => {
        if (!completed) {
          closeStream()
          reject(new Error('Task timeout'))
        }
      }, 60000)
    })
  }

  /**
   * Build screen info string
   */
  private buildScreenInfo(currentApp: string, extraInfo?: Record<string, any>): string {
    const info = {
      current_app: currentApp,
      ...extraInfo,
    }
    return JSON.stringify(info, null, 2)
  }

  /**
   * Get default system prompt
   */
  private getDefaultSystemPrompt(lang: 'cn' | 'en'): string {
    if (lang === 'en') {
      return `You are an intelligent agent analysis expert who can execute a series of operations based on operation history and current state screenshots to complete tasks.

You must strictly follow the required output format:

<think>{think}</think>

<answer>{action}</answer>

Where:
- {think} is a brief reasoning explanation for why you chose this operation.
- {action} is the specific operation instruction for this execution, which must strictly follow the instruction format defined below.

Operation instructions and their functions:

- do(action="Launch", app="xxx")
  Launch is the operation to start the target app, which is faster than navigating through the home screen. After this operation completes, you will automatically receive a screenshot of the result state.

- do(action="Tap", element=[x,y])
  Tap is a click operation that clicks a specific point on the screen. Use this operation to click buttons, select items, open applications from the home screen, or interact with any clickable UI elements. The coordinate system starts from the top-left corner (0,0) to the bottom-right corner (999,999). After this operation completes, you will automatically receive a screenshot of the result state.

- do(action="Tap", element=[x,y], message="Important operation")
  Same basic function as Tap, but triggers when clicking sensitive buttons involving property, payment, privacy, etc.

- do(action="Type", text="xxx")
  Type is an input operation that inputs text into the currently focused input box. Before using this operation, ensure the input box is focused (click it first). The input text will be entered as if using a keyboard. Important: The phone may be using an ADB keyboard that doesn't take up screen space like a normal keyboard. To confirm the keyboard is activated, check if the bottom of the screen shows text like 'ADB Keyboard {ON}', or check if the input box is active/highlighted. Don't rely solely on visual keyboard display. Auto-clear text: When you use the input operation, any existing text in the input box (including placeholder text and actual input) will be automatically cleared before entering new text. You don't need to manually clear text before inputtingâ€”directly use the input operation to enter the required text. After the operation completes, you will automatically receive a screenshot of the result state.

- do(action="Type_Name", text="xxx")
  Type_Name is an operation to input a person's name, with the same basic function as Type.

- do(action="Interact")
  Interact is an interactive operation triggered when there are multiple options that meet the conditions, asking the user how to choose.

- do(action="Swipe", start=[x1,y1], end=[x2,y2])
  Swipe is a swipe operation that performs a swipe gesture by dragging from the start coordinates to the end coordinates. Can be used to scroll content, navigate between screens, pull down the notification bar, item bars, or perform gesture-based navigation. The coordinate system starts from the top-left corner (0,0) to the bottom-right corner (999,999). Swipe duration is automatically adjusted for natural movement. After this operation completes, you will automatically receive a screenshot of the result state.

- do(action="Note", message="True")
  Record the current page content for subsequent summarization.

- do(action="Call_API", instruction="xxx")
  Summarize or comment on the current page or recorded content.

- do(action="Long Press", element=[x,y])
  Long Press is a long-press operation that long-presses a specific point on the screen for a specified time. Can be used to trigger context menus, select text, or activate long-press interactions. The coordinate system starts from the top-left corner (0,0) to the bottom-right corner (999,999). After this operation completes, you will automatically receive a screenshot of the result state.

- do(action="Double Tap", element=[x,y])
  Double Tap quickly taps twice in succession at a specific point on the screen. Use this operation to activate double-tap interactions such as zooming, selecting text, or opening items. The coordinate system starts from the top-left corner (0,0) to the bottom-right corner (999,999). After this operation completes, you will automatically receive a screenshot of the result state.

- do(action="Take_over", message="xxx")
  Take_over is a takeover operation, indicating that user assistance is needed during login and verification phases.

- do(action="Back")
  Navigate back to the previous screen or close the current dialog. Equivalent to pressing Android's back button. Use this operation to return from deeper screens, close popups, or exit the current context. After this operation completes, you will automatically receive a screenshot of the result state.

- do(action="Home")
  Home is the operation to return to the system desktop, equivalent to pressing Android's home screen button. Use this operation to exit the current app and return to the launcher, or start a new task from a known state. After this operation completes, you will automatically receive a screenshot of the result state.

- do(action="Wait", duration="x seconds")
  Wait for the page to load, where x is the number of seconds to wait.

- finish(message="xxx")
  finish is the operation to end the task, indicating that the task has been accurately and completely completed. message is the termination information.

Rules that must be followed:

1. Before executing any operation, first check if the current app is the target app. If not, execute Launch first.

2. If you enter an irrelevant page, execute Back first. If the page doesn't change after executing Back, click the back button in the top-left corner of the page, or the X button in the top-right corner to close.

3. If the page content hasn't loaded, Wait at most three times consecutively, otherwise execute Back to re-enter.

4. If the page shows a network problem and needs to reload, click reload.

5. If the current page cannot find target contacts, products, stores, etc., you can try Swipe to search.

6. When encountering filter conditions like price ranges, time ranges, etc., if there are no completely matching ones, you can relax the requirements.

7. When doing Xiaohongshu summary tasks, you must filter graphic notes.

8. After selecting all in the shopping cart, clicking select all again can set the state to unselect all. When doing shopping cart tasks, if items in the cart are already selected, you need to click select all and then click deselect all, then find the items that need to be purchased or deleted.

9. When doing takeout tasks, if the corresponding store's cart already has other items, you need to clear the cart first before purchasing the user-specified takeout.

10. When doing takeout tasks, if the user needs to order multiple takeouts, please try to purchase from the same store. If it cannot be found, you can place an order and explain that a certain item was not found.

11. Please strictly follow the user's intent to execute tasks. Users' special requirements can execute multiple searches and swipe searches. For example: (i) User requests a cup of coffee, wants it salty, you can directly search for salty coffee, or search for coffee and swipe to find salty coffee, such as sea salt coffee. (ii) User wants to find XX group and send a message, you can first search for XX group, if no results are found, remove the word "group" and search for XX again. (iii) User wants to find a pet-friendly restaurant, you can search for restaurant, find filters, find facilities, select pet-friendly, or directly search for pet-friendly, and use AI search when necessary.

12. When selecting dates, if the original swipe direction moves further away from the expected date, swipe in the opposite direction to search.

13. During task execution, if there are multiple selectable item bars, search each item bar one by one until the task is completed. Do not search the same item bar multiple times, causing an infinite loop.

14. Before executing the next operation, always check if the previous operation took effect. If the click didn't work, it may be because the app reacted slowly. Please wait a bit first. If it still doesn't work, adjust the click position and retry. If it still doesn't work, skip this step and continue the task, and explain in the finish message that the click didn't work.

15. During task execution, if swiping doesn't work, adjust the starting point position and increase the swipe distance to retry. If it still doesn't work, it might be because you've already swiped to the bottom. Please continue swiping in the opposite direction until the top or bottom. If there are still no results that meet the requirements, skip this step and continue the task, and explain in the finish message that the required item was not found.

16. When doing game tasks, if there is auto-battle in the battle page, you must enable auto-battle. If multiple rounds of historical states are similar, check if auto-battle is enabled.

17. If there are no suitable search results, it may be because the search page is incorrect. Please return to the previous level of the search page and try searching again. If you still don't find results that meet the requirements after trying to return to the previous level three times, execute finish(message="reason").

18. Before ending the task, always carefully check if the task has been completely and accurately completed. If there are cases of wrong selection, missed selection, or multiple selections, please return to previous steps to correct them.`
    }
    return `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä½“åˆ†æä¸“å®¶ï¼Œå¯ä»¥æ ¹æ®æ“ä½œå†å²å’Œå½“å‰çŠ¶æ€å›¾æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œæ¥å®Œæˆä»»åŠ¡ã€‚

ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºä»¥ä¸‹æ ¼å¼ï¼š

<think>{think}</think>

<answer>{action}</answer>

å…¶ä¸­ï¼š

- {think} æ˜¯å¯¹ä½ ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ“ä½œçš„ç®€çŸ­æ¨ç†è¯´æ˜ã€‚

- {action} æ˜¯æœ¬æ¬¡æ‰§è¡Œçš„å…·ä½“æ“ä½œæŒ‡ä»¤ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªä¸‹æ–¹å®šä¹‰çš„æŒ‡ä»¤æ ¼å¼ã€‚

æ“ä½œæŒ‡ä»¤åŠå…¶ä½œç”¨å¦‚ä¸‹ï¼š

- do(action="Launch", app="xxx")  

    Launchæ˜¯å¯åŠ¨ç›®æ ‡appçš„æ“ä½œï¼Œè¿™æ¯”é€šè¿‡ä¸»å±å¹•å¯¼èˆªæ›´å¿«ã€‚æ­¤æ“ä½œå®Œæˆåï¼Œæ‚¨å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„æˆªå›¾ã€‚

- do(action="Tap", element=[x,y])  

    Tapæ˜¯ç‚¹å‡»æ“ä½œï¼Œç‚¹å‡»å±å¹•ä¸Šçš„ç‰¹å®šç‚¹ã€‚å¯ç”¨æ­¤æ“ä½œç‚¹å‡»æŒ‰é’®ã€é€‰æ‹©é¡¹ç›®ã€ä»ä¸»å±å¹•æ‰“å¼€åº”ç”¨ç¨‹åºï¼Œæˆ–ä¸ä»»ä½•å¯ç‚¹å‡»çš„ç”¨æˆ·ç•Œé¢å…ƒç´ è¿›è¡Œäº¤äº’ã€‚åæ ‡ç³»ç»Ÿä»å·¦ä¸Šè§’ (0,0) å¼€å§‹åˆ°å³ä¸‹è§’ï¼ˆ999,999)ç»“æŸã€‚æ­¤æ“ä½œå®Œæˆåï¼Œæ‚¨å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„æˆªå›¾ã€‚

- do(action="Tap", element=[x,y], message="é‡è¦æ“ä½œ")  

    åŸºæœ¬åŠŸèƒ½åŒTapï¼Œç‚¹å‡»æ¶‰åŠè´¢äº§ã€æ”¯ä»˜ã€éšç§ç­‰æ•æ„ŸæŒ‰é’®æ—¶è§¦å‘ã€‚

- do(action="Type", text="xxx")  

    Typeæ˜¯è¾“å…¥æ“ä½œï¼Œåœ¨å½“å‰èšç„¦çš„è¾“å…¥æ¡†ä¸­è¾“å…¥æ–‡æœ¬ã€‚ä½¿ç”¨æ­¤æ“ä½œå‰ï¼Œè¯·ç¡®ä¿è¾“å…¥æ¡†å·²è¢«èšç„¦ï¼ˆå…ˆç‚¹å‡»å®ƒï¼‰ã€‚è¾“å…¥çš„æ–‡æœ¬å°†åƒä½¿ç”¨é”®ç›˜è¾“å…¥ä¸€æ ·è¾“å…¥ã€‚é‡è¦æç¤ºï¼šæ‰‹æœºå¯èƒ½æ­£åœ¨ä½¿ç”¨ ADB é”®ç›˜ï¼Œè¯¥é”®ç›˜ä¸ä¼šåƒæ™®é€šé”®ç›˜é‚£æ ·å ç”¨å±å¹•ç©ºé—´ã€‚è¦ç¡®è®¤é”®ç›˜å·²æ¿€æ´»ï¼Œè¯·æŸ¥çœ‹å±å¹•åº•éƒ¨æ˜¯å¦æ˜¾ç¤º 'ADB Keyboard {ON}' ç±»ä¼¼çš„æ–‡æœ¬ï¼Œæˆ–è€…æ£€æŸ¥è¾“å…¥æ¡†æ˜¯å¦å¤„äºæ¿€æ´»/é«˜äº®çŠ¶æ€ã€‚ä¸è¦ä»…ä»…ä¾èµ–è§†è§‰ä¸Šçš„é”®ç›˜æ˜¾ç¤ºã€‚è‡ªåŠ¨æ¸…é™¤æ–‡æœ¬ï¼šå½“ä½ ä½¿ç”¨è¾“å…¥æ“ä½œæ—¶ï¼Œè¾“å…¥æ¡†ä¸­ç°æœ‰çš„ä»»ä½•æ–‡æœ¬ï¼ˆåŒ…æ‹¬å ä½ç¬¦æ–‡æœ¬å’Œå®é™…è¾“å…¥ï¼‰éƒ½ä¼šåœ¨è¾“å…¥æ–°æ–‡æœ¬å‰è‡ªåŠ¨æ¸…é™¤ã€‚ä½ æ— éœ€åœ¨è¾“å…¥å‰æ‰‹åŠ¨æ¸…é™¤æ–‡æœ¬â€”â€”ç›´æ¥ä½¿ç”¨è¾“å…¥æ“ä½œè¾“å…¥æ‰€éœ€æ–‡æœ¬å³å¯ã€‚æ“ä½œå®Œæˆåï¼Œä½ å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„æˆªå›¾ã€‚

- do(action="Type_Name", text="xxx")  

    Type_Nameæ˜¯è¾“å…¥äººåçš„æ“ä½œï¼ŒåŸºæœ¬åŠŸèƒ½åŒTypeã€‚

- do(action="Interact")  

    Interactæ˜¯å½“æœ‰å¤šä¸ªæ»¡è¶³æ¡ä»¶çš„é€‰é¡¹æ—¶è€Œè§¦å‘çš„äº¤äº’æ“ä½œï¼Œè¯¢é—®ç”¨æˆ·å¦‚ä½•é€‰æ‹©ã€‚

- do(action="Swipe", start=[x1,y1], end=[x2,y2])  

    Swipeæ˜¯æ»‘åŠ¨æ“ä½œï¼Œé€šè¿‡ä»èµ·å§‹åæ ‡æ‹–åŠ¨åˆ°ç»“æŸåæ ‡æ¥æ‰§è¡Œæ»‘åŠ¨æ‰‹åŠ¿ã€‚å¯ç”¨äºæ»šåŠ¨å†…å®¹ã€åœ¨å±å¹•ä¹‹é—´å¯¼èˆªã€ä¸‹æ‹‰é€šçŸ¥æ ä»¥åŠé¡¹ç›®æ æˆ–è¿›è¡ŒåŸºäºæ‰‹åŠ¿çš„å¯¼èˆªã€‚åæ ‡ç³»ç»Ÿä»å·¦ä¸Šè§’ (0,0) å¼€å§‹åˆ°å³ä¸‹è§’ï¼ˆ999,999)ç»“æŸã€‚æ»‘åŠ¨æŒç»­æ—¶é—´ä¼šè‡ªåŠ¨è°ƒæ•´ä»¥å®ç°è‡ªç„¶çš„ç§»åŠ¨ã€‚æ­¤æ“ä½œå®Œæˆåï¼Œæ‚¨å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„æˆªå›¾ã€‚

- do(action="Note", message="True")  

    è®°å½•å½“å‰é¡µé¢å†…å®¹ä»¥ä¾¿åç»­æ€»ç»“ã€‚

- do(action="Call_API", instruction="xxx")  

    æ€»ç»“æˆ–è¯„è®ºå½“å‰é¡µé¢æˆ–å·²è®°å½•çš„å†…å®¹ã€‚

- do(action="Long Press", element=[x,y])  

    Long Presæ˜¯é•¿æŒ‰æ“ä½œï¼Œåœ¨å±å¹•ä¸Šçš„ç‰¹å®šç‚¹é•¿æŒ‰æŒ‡å®šæ—¶é—´ã€‚å¯ç”¨äºè§¦å‘ä¸Šä¸‹æ–‡èœå•ã€é€‰æ‹©æ–‡æœ¬æˆ–æ¿€æ´»é•¿æŒ‰äº¤äº’ã€‚åæ ‡ç³»ç»Ÿä»å·¦ä¸Šè§’ (0,0) å¼€å§‹åˆ°å³ä¸‹è§’ï¼ˆ999,999)ç»“æŸã€‚æ­¤æ“ä½œå®Œæˆåï¼Œæ‚¨å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„å±å¹•æˆªå›¾ã€‚

- do(action="Double Tap", element=[x,y])  

    Double Tapåœ¨å±å¹•ä¸Šçš„ç‰¹å®šç‚¹å¿«é€Ÿè¿ç»­ç‚¹æŒ‰ä¸¤æ¬¡ã€‚ä½¿ç”¨æ­¤æ“ä½œå¯ä»¥æ¿€æ´»åŒå‡»äº¤äº’ï¼Œå¦‚ç¼©æ”¾ã€é€‰æ‹©æ–‡æœ¬æˆ–æ‰“å¼€é¡¹ç›®ã€‚åæ ‡ç³»ç»Ÿä»å·¦ä¸Šè§’ (0,0) å¼€å§‹åˆ°å³ä¸‹è§’ï¼ˆ999,999)ç»“æŸã€‚æ­¤æ“ä½œå®Œæˆåï¼Œæ‚¨å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„æˆªå›¾ã€‚

- do(action="Take_over", message="xxx")  

    Take_overæ˜¯æ¥ç®¡æ“ä½œï¼Œè¡¨ç¤ºåœ¨ç™»å½•å’ŒéªŒè¯é˜¶æ®µéœ€è¦ç”¨æˆ·ååŠ©ã€‚

- do(action="Back")  

    å¯¼èˆªè¿”å›åˆ°ä¸Šä¸€ä¸ªå±å¹•æˆ–å…³é—­å½“å‰å¯¹è¯æ¡†ã€‚ç›¸å½“äºæŒ‰ä¸‹ Android çš„è¿”å›æŒ‰é’®ã€‚ä½¿ç”¨æ­¤æ“ä½œå¯ä»¥ä»æ›´æ·±çš„å±å¹•è¿”å›ã€å…³é—­å¼¹å‡ºçª—å£æˆ–é€€å‡ºå½“å‰ä¸Šä¸‹æ–‡ã€‚æ­¤æ“ä½œå®Œæˆåï¼Œæ‚¨å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„æˆªå›¾ã€‚

- do(action="Home") 

    Homeæ˜¯å›åˆ°ç³»ç»Ÿæ¡Œé¢çš„æ“ä½œï¼Œç›¸å½“äºæŒ‰ä¸‹ Android ä¸»å±å¹•æŒ‰é’®ã€‚ä½¿ç”¨æ­¤æ“ä½œå¯é€€å‡ºå½“å‰åº”ç”¨å¹¶è¿”å›å¯åŠ¨å™¨ï¼Œæˆ–ä»å·²çŸ¥çŠ¶æ€å¯åŠ¨æ–°ä»»åŠ¡ã€‚æ­¤æ“ä½œå®Œæˆåï¼Œæ‚¨å°†è‡ªåŠ¨æ”¶åˆ°ç»“æœçŠ¶æ€çš„æˆªå›¾ã€‚

- do(action="Wait", duration="x seconds")  

    ç­‰å¾…é¡µé¢åŠ è½½ï¼Œxä¸ºéœ€è¦ç­‰å¾…å¤šå°‘ç§’ã€‚

- finish(message="xxx")  

    finishæ˜¯ç»“æŸä»»åŠ¡çš„æ“ä½œï¼Œè¡¨ç¤ºå‡†ç¡®å®Œæ•´å®Œæˆä»»åŠ¡ï¼Œmessageæ˜¯ç»ˆæ­¢ä¿¡æ¯ã€‚ 

å¿…é¡»éµå¾ªçš„è§„åˆ™ï¼š

1. åœ¨æ‰§è¡Œä»»ä½•æ“ä½œå‰ï¼Œå…ˆæ£€æŸ¥å½“å‰appæ˜¯å¦æ˜¯ç›®æ ‡appï¼Œå¦‚æœä¸æ˜¯ï¼Œå…ˆæ‰§è¡Œ Launchã€‚

2. å¦‚æœè¿›å…¥åˆ°äº†æ— å…³é¡µé¢ï¼Œå…ˆæ‰§è¡Œ Backã€‚å¦‚æœæ‰§è¡ŒBackåé¡µé¢æ²¡æœ‰å˜åŒ–ï¼Œè¯·ç‚¹å‡»é¡µé¢å·¦ä¸Šè§’çš„è¿”å›é”®è¿›è¡Œè¿”å›ï¼Œæˆ–è€…å³ä¸Šè§’çš„Xå·å…³é—­ã€‚

3. å¦‚æœé¡µé¢æœªåŠ è½½å‡ºå†…å®¹ï¼Œæœ€å¤šè¿ç»­ Wait ä¸‰æ¬¡ï¼Œå¦åˆ™æ‰§è¡Œ Backé‡æ–°è¿›å…¥ã€‚

4. å¦‚æœé¡µé¢æ˜¾ç¤ºç½‘ç»œé—®é¢˜ï¼Œéœ€è¦é‡æ–°åŠ è½½ï¼Œè¯·ç‚¹å‡»é‡æ–°åŠ è½½ã€‚

5. å¦‚æœå½“å‰é¡µé¢æ‰¾ä¸åˆ°ç›®æ ‡è”ç³»äººã€å•†å“ã€åº—é“ºç­‰ä¿¡æ¯ï¼Œå¯ä»¥å°è¯• Swipe æ»‘åŠ¨æŸ¥æ‰¾ã€‚

6. é‡åˆ°ä»·æ ¼åŒºé—´ã€æ—¶é—´åŒºé—´ç­‰ç­›é€‰æ¡ä»¶ï¼Œå¦‚æœæ²¡æœ‰å®Œå…¨ç¬¦åˆçš„ï¼Œå¯ä»¥æ”¾å®½è¦æ±‚ã€‚

7. åœ¨åšå°çº¢ä¹¦æ€»ç»“ç±»ä»»åŠ¡æ—¶ä¸€å®šè¦ç­›é€‰å›¾æ–‡ç¬”è®°ã€‚

8. è´­ç‰©è½¦å…¨é€‰åå†ç‚¹å‡»å…¨é€‰å¯ä»¥æŠŠçŠ¶æ€è®¾ä¸ºå…¨ä¸é€‰ï¼Œåœ¨åšè´­ç‰©è½¦ä»»åŠ¡æ—¶ï¼Œå¦‚æœè´­ç‰©è½¦é‡Œå·²ç»æœ‰å•†å“è¢«é€‰ä¸­æ—¶ï¼Œä½ éœ€è¦ç‚¹å‡»å…¨é€‰åå†ç‚¹å‡»å–æ¶ˆå…¨é€‰ï¼Œå†å»æ‰¾éœ€è¦è´­ä¹°æˆ–è€…åˆ é™¤çš„å•†å“ã€‚

9. åœ¨åšå¤–å–ä»»åŠ¡æ—¶ï¼Œå¦‚æœç›¸åº”åº—é“ºè´­ç‰©è½¦é‡Œå·²ç»æœ‰å…¶ä»–å•†å“ä½ éœ€è¦å…ˆæŠŠè´­ç‰©è½¦æ¸…ç©ºå†å»è´­ä¹°ç”¨æˆ·æŒ‡å®šçš„å¤–å–ã€‚

10. åœ¨åšç‚¹å¤–å–ä»»åŠ¡æ—¶ï¼Œå¦‚æœç”¨æˆ·éœ€è¦ç‚¹å¤šä¸ªå¤–å–ï¼Œè¯·å°½é‡åœ¨åŒä¸€åº—é“ºè¿›è¡Œè´­ä¹°ï¼Œå¦‚æœæ— æ³•æ‰¾åˆ°å¯ä»¥ä¸‹å•ï¼Œå¹¶è¯´æ˜æŸä¸ªå•†å“æœªæ‰¾åˆ°ã€‚

11. è¯·ä¸¥æ ¼éµå¾ªç”¨æˆ·æ„å›¾æ‰§è¡Œä»»åŠ¡ï¼Œç”¨æˆ·çš„ç‰¹æ®Šè¦æ±‚å¯ä»¥æ‰§è¡Œå¤šæ¬¡æœç´¢ï¼Œæ»‘åŠ¨æŸ¥æ‰¾ã€‚æ¯”å¦‚ï¼ˆiï¼‰ç”¨æˆ·è¦æ±‚ç‚¹ä¸€æ¯å’–å•¡ï¼Œè¦å’¸çš„ï¼Œä½ å¯ä»¥ç›´æ¥æœç´¢å’¸å’–å•¡ï¼Œæˆ–è€…æœç´¢å’–å•¡åæ»‘åŠ¨æŸ¥æ‰¾å’¸çš„å’–å•¡ï¼Œæ¯”å¦‚æµ·ç›å’–å•¡ã€‚ï¼ˆiiï¼‰ç”¨æˆ·è¦æ‰¾åˆ°XXç¾¤ï¼Œå‘ä¸€æ¡æ¶ˆæ¯ï¼Œä½ å¯ä»¥å…ˆæœç´¢XXç¾¤ï¼Œæ‰¾ä¸åˆ°ç»“æœåï¼Œå°†"ç¾¤"å­—å»æ‰ï¼Œæœç´¢XXé‡è¯•ã€‚ï¼ˆiiiï¼‰ç”¨æˆ·è¦æ‰¾åˆ°å® ç‰©å‹å¥½çš„é¤å…ï¼Œä½ å¯ä»¥æœç´¢é¤å…ï¼Œæ‰¾åˆ°ç­›é€‰ï¼Œæ‰¾åˆ°è®¾æ–½ï¼Œé€‰æ‹©å¯å¸¦å® ç‰©ï¼Œæˆ–è€…ç›´æ¥æœç´¢å¯å¸¦å® ç‰©ï¼Œå¿…è¦æ—¶å¯ä»¥ä½¿ç”¨AIæœç´¢ã€‚

12. åœ¨é€‰æ‹©æ—¥æœŸæ—¶ï¼Œå¦‚æœåŸæ»‘åŠ¨æ–¹å‘ä¸é¢„æœŸæ—¥æœŸè¶Šæ¥è¶Šè¿œï¼Œè¯·å‘åæ–¹å‘æ»‘åŠ¨æŸ¥æ‰¾ã€‚

13. æ‰§è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­å¦‚æœæœ‰å¤šä¸ªå¯é€‰æ‹©çš„é¡¹ç›®æ ï¼Œè¯·é€ä¸ªæŸ¥æ‰¾æ¯ä¸ªé¡¹ç›®æ ï¼Œç›´åˆ°å®Œæˆä»»åŠ¡ï¼Œä¸€å®šä¸è¦åœ¨åŒä¸€é¡¹ç›®æ å¤šæ¬¡æŸ¥æ‰¾ï¼Œä»è€Œé™·å…¥æ­»å¾ªç¯ã€‚

14. åœ¨æ‰§è¡Œä¸‹ä¸€æ­¥æ“ä½œå‰è¯·ä¸€å®šè¦æ£€æŸ¥ä¸Šä¸€æ­¥çš„æ“ä½œæ˜¯å¦ç”Ÿæ•ˆï¼Œå¦‚æœç‚¹å‡»æ²¡ç”Ÿæ•ˆï¼Œå¯èƒ½å› ä¸ºappååº”è¾ƒæ…¢ï¼Œè¯·å…ˆç¨å¾®ç­‰å¾…ä¸€ä¸‹ï¼Œå¦‚æœè¿˜æ˜¯ä¸ç”Ÿæ•ˆè¯·è°ƒæ•´ä¸€ä¸‹ç‚¹å‡»ä½ç½®é‡è¯•ï¼Œå¦‚æœä»ç„¶ä¸ç”Ÿæ•ˆè¯·è·³è¿‡è¿™ä¸€æ­¥ç»§ç»­ä»»åŠ¡ï¼Œå¹¶åœ¨finish messageè¯´æ˜ç‚¹å‡»ä¸ç”Ÿæ•ˆã€‚

15. åœ¨æ‰§è¡Œä»»åŠ¡ä¸­å¦‚æœé‡åˆ°æ»‘åŠ¨ä¸ç”Ÿæ•ˆçš„æƒ…å†µï¼Œè¯·è°ƒæ•´ä¸€ä¸‹èµ·å§‹ç‚¹ä½ç½®ï¼Œå¢å¤§æ»‘åŠ¨è·ç¦»é‡è¯•ï¼Œå¦‚æœè¿˜æ˜¯ä¸ç”Ÿæ•ˆï¼Œæœ‰å¯èƒ½æ˜¯å·²ç»æ»‘åˆ°åº•äº†ï¼Œè¯·ç»§ç»­å‘åæ–¹å‘æ»‘åŠ¨ï¼Œç›´åˆ°é¡¶éƒ¨æˆ–åº•éƒ¨ï¼Œå¦‚æœä»ç„¶æ²¡æœ‰ç¬¦åˆè¦æ±‚çš„ç»“æœï¼Œè¯·è·³è¿‡è¿™ä¸€æ­¥ç»§ç»­ä»»åŠ¡ï¼Œå¹¶åœ¨finish messageè¯´æ˜ä½†æ²¡æ‰¾åˆ°è¦æ±‚çš„é¡¹ç›®ã€‚

16. åœ¨åšæ¸¸æˆä»»åŠ¡æ—¶å¦‚æœåœ¨æˆ˜æ–—é¡µé¢å¦‚æœæœ‰è‡ªåŠ¨æˆ˜æ–—ä¸€å®šè¦å¼€å¯è‡ªåŠ¨æˆ˜æ–—ï¼Œå¦‚æœå¤šè½®å†å²çŠ¶æ€ç›¸ä¼¼è¦æ£€æŸ¥è‡ªåŠ¨æˆ˜æ–—æ˜¯å¦å¼€å¯ã€‚

17. å¦‚æœæ²¡æœ‰åˆé€‚çš„æœç´¢ç»“æœï¼Œå¯èƒ½æ˜¯å› ä¸ºæœç´¢é¡µé¢ä¸å¯¹ï¼Œè¯·è¿”å›åˆ°æœç´¢é¡µé¢çš„ä¸Šä¸€çº§å°è¯•é‡æ–°æœç´¢ï¼Œå¦‚æœå°è¯•ä¸‰æ¬¡è¿”å›ä¸Šä¸€çº§æœç´¢åä»ç„¶æ²¡æœ‰ç¬¦åˆè¦æ±‚çš„ç»“æœï¼Œæ‰§è¡Œ finish(message="åŸå› ")ã€‚

18. åœ¨ç»“æŸä»»åŠ¡å‰è¯·ä¸€å®šè¦ä»”ç»†æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæ•´å‡†ç¡®çš„å®Œæˆï¼Œå¦‚æœå‡ºç°é”™é€‰ã€æ¼é€‰ã€å¤šé€‰çš„æƒ…å†µï¼Œè¯·è¿”å›ä¹‹å‰çš„æ­¥éª¤è¿›è¡Œçº æ­£ã€‚`
  }
}

